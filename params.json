{"name":"Reality-of-sqoop-mainframe","tagline":"","body":"### Sqoop-mainframe\r\nSyncsort submitted a patch (SQOOP-1272) to extend Sqoop for transferring data from Mainframe to Hadoop, allowing multiple Mainframe data sets to be moved to HDFS in parallel. \r\n- See more at: http://blog.syncsort.com/2014/06/big-iron-big-data-mainframe-hadoop-apache-sqoop/#sthash.e6MHJ7dd.dpuf\r\n\r\nHow to use the utility is detailed at: https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_literal_sqoop_import_mainframe_literal\r\n\r\n### What is missing?\r\nHowever based on my validation dated 10/27/2015, below are the some of the important features  that are missed.\r\n\r\n1. Sqoop-mainframe can only handle Fixed length files, variable length files are not supported. \r\n1. Only folder structures like data can be downloaded i.e., PDS and Fixed length GDGs as a whole. \r\n1. EBCIDIC to ASCII conversion return invalid data when the data has computational fields.\r\n\r\nIf your target file needs to be in hive I would suggest you an alternate approach using hive custom serde.\r\n- See more at: http://rbheemana.github.io/Cobol-to-Hive/serde.html\r\n\r\n### GDG Issue\r\nTo be more specific lets look at a common scenario. \r\nA Mainframe system updates daily/monthly activities in GDG file. Usually a new version of GDG is created, for that day/month and previous day/month will be in earlier version.\r\nExample of such dataset in Mainframe will look like this:\r\n\r\n       MF.FBFILE.GDG  \t      -- Base\r\n       MF.FBFILE.GDG.G0001V00  -- 10/27/2015 data\r\n       MF.FBFILE.GDG.G0002V00  -- 10/28/2015 data\r\n       MF.FBFILE.GDG.G0003V00  -- 10/29/2015 data\r\n       MF.FBFILE.GDG.G0004V00  -- 10/30/2015 data\r\n       MF.FBFILE.GDG.G0005V00  -- 10/31/2015 data\r\n\r\nSqoop-mainframe utility will allow us to download the entire GDG meaning all the data (5 days in our example). But in most of our scenarios we are interested only in today's data latest file. Sqoop mainframe will not allow to get a single file, it is all or none.\r\nMainframe system usually stores year worth of data in these GDGs which makes it not worth all the files when we actually need single day data.\r\nIn mainframe terminology, we can use MF.FBFILE.GDG(0) to get latest version every day and MF.FBFILE.GDG(-1) to get previous day data and so on. This convention is not supported in sqoop-mainframe, which makes us to write complex logic to get the file version number (such as G0005V00).\r\n\r\n### EBCDIC to ASCII Issue\r\nSqoop-mainframe automatically converts EBCDIC data to ASCII and then stores on the hadoop. This sounds cool but this fails when the data have computational fileds.\r\nAlmost every file we need from mainframe will computational fields(COMP, COMP-3 etc). In these scenarios, sqoop-mainframe EBCIDIC To ASCII conversion interprets the data incorrectly and returns unreadable characters.\r\nTo interpret mainframe data correctly, we need to have its corresponding cobol copybook layout. Through which we have to exclude the computational fields data and then convert rest of the data from EBCIDIC to ASCII.\r\nComputational fields needs to be seperately converted based on their representation, usually these fields are represented in binary or big-endian format.\r\n\r\n### Variable length files issue\r\nMost important feature support for variable length files is missing in sqoop-mainframe. If we try to give VB length file, sqoop-mainframe return as if that file doesnot exist on mainframe\r\n\r\n### Work-around or How to make it work?\r\nSo if anyone is planning to use sqoop-mainframe one needs to write a mainframe job  \r\n- to convert VB files to FB files \r\n- to convert computational fields to text\r\n- place the updated file in GDG with only one version\r\n- make sure to delete the older versions in GDG.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}